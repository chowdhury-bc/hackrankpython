import os
import boto3
import argparse
import torch
import pandas as pd
from torch.utils.data import DataLoader
from tqdm import tqdm
import timm
from Datasets import Dataset
from EfficientNet_model import EfficientNet

# Initialize S3 client
s3_client = boto3.client('s3')

def ensure_directory_exists(directory):
    print(f"Ensuring directory exists for: {directory}")
    if not os.path.exists(directory):
        print(f"Directory does not exist. Creating directory: {directory}")
        os.makedirs(directory)
    else:
        print(f"Directory already exists: {directory}")

def download_file_from_s3(bucket_name, s3_key, local_dir):
    ensure_directory_exists(local_dir)
    local_file_path = os.path.join(local_dir, os.path.basename(s3_key))
    print(f"Downloading file from S3: Bucket={bucket_name}, Key={s3_key}, LocalPath={local_file_path}")
    s3_client.download_file(bucket_name, s3_key, local_file_path)
    print(f"Downloaded file from S3 to {local_file_path}")
    return local_file_path

def get_latest_file_from_s3(bucket_name, prefix):
    print(f"Listing objects in S3: Bucket={bucket_name}, Prefix={prefix}")
    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
    if 'Contents' not in response:
        raise ValueError("No files found in the bucket.")
    
    files = response['Contents']
    latest_file = max(files, key=lambda x: x['LastModified'])
    print(f"Latest file in S3: {latest_file['Key']}")
    return latest_file['Key']

def generate_predictions(images, model, device):
    images = images.float().to(device)
    with torch.no_grad():
        outputs = model(images).clone().detach().cpu()
    return outputs

def append_predictions_to_csv(predictions, filepaths, local_output_path):
    # Convert predictions to DataFrame
    predictions_df = pd.DataFrame(predictions.numpy(), columns=['Prediction'])
    predictions_df['Filepath'] = filepaths
    if os.path.exists(local_output_path):
        existing_df = pd.read_csv(local_output_path)
        combined_df = pd.concat([existing_df, predictions_df], ignore_index=True)
    else:
        combined_df = predictions_df
    combined_df.to_csv(local_output_path, index=False)
    print(f"Appended predictions to CSV at {local_output_path}")

def check_file_integrity(file_path):
    try:
        file_size = os.path.getsize(file_path)
        print(f"File size: {file_size} bytes")
        if file_size == 0:
            raise ValueError("Downloaded file is empty.")
    except Exception as e:
        print(f"Error checking file integrity: {e}")
        raise

if __name__ == '__main__':
    # Argparsing
    parser = argparse.ArgumentParser(description='Train a model.')
    parser.add_argument('--input_img_size', type=int, default=800,  help='The input image size.')
    parser.add_argument('--batch_size',     type=int, default=1,    help='The batch size of input images.')
    parser.add_argument('--model_path',     type=str, default=None, help='The path to a saved model to use.')
    parser.add_argument('--image_path',     type=str, default=None, help='The path to an image or folder of images')
    parser.add_argument('--bucket_name',    type=str, required=True, help='The S3 bucket name.')
    parser.add_argument('--input_s3_prefix', type=str, required=True, help='The S3 prefix for the input images.')
    parser.add_argument('--local_dir', type=str, required=True, help='The local directory to save the downloaded image.')
    parser.add_argument('--output_s3_key',  type=str, default=None, help='The S3 key for the output CSV.')
    args = parser.parse_args()

    input_img_size = args.input_img_size
    batch_size = args.batch_size
    model_path = args.model_path
    image_path = args.image_path
    bucket_name = args.bucket_name
    input_s3_prefix = args.input_s3_prefix
    local_dir = args.local_dir
    output_s3_key = args.output_s3_key

    torch.backends.cudnn.benchmark = True
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model = EfficientNet(device, model_path, param_freeze_ratio=0.66)

    # Get the latest file from S3
    latest_file_key = get_latest_file_from_s3(bucket_name, input_s3_prefix)
    local_image_path = download_file_from_s3(bucket_name, latest_file_key, local_dir)

    # Verify the downloaded file
    check_file_integrity(local_image_path)

    # Use the downloaded image
    image_files = [local_image_path]

    dataset_params = {
        'model_config': timm.data.resolve_model_data_config(model),
        'input_img_size': input_img_size,
        'ROI': ((1067,417),(2393,1743))  # Example ROI
    }

    try:
        inference_dataset = Dataset(image_files, **dataset_params, scaler=model.scaler, training=False)
        dataloader = DataLoader(inference_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)

        model.eval()
        predictions = []
        filepaths = []
        for i, (images, _) in tqdm(enumerate(dataloader), total=len(dataloader)):
            print(f"Processing batch {i+1}/{len(dataloader)}")
            outputs = generate_predictions(images, model, device)
            if inference_dataset.scaler:
                outputs = inference_dataset.reverse_scale(outputs)
            else:
                print("Scaler is None, skipping inverse transformation")

            predictions.extend(outputs)
            filepaths.extend(inference_dataset.image_paths[i*batch_size:(i+1)*batch_size])

        # Local path to save the predictions CSV
        local_output_path = '/tmp/inference_predictions.csv'

        # Append predictions to CSV
        append_predictions_to_csv(predictions, filepaths, local_output_path)

        # Upload the predictions CSV back to S3
        upload_file_to_s3(local_output_path, bucket_name, output_s3_key)
    except Exception as e:
        print(f"Error during processing: {e}")
