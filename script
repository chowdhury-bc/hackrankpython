import argparse
import pickle
from statistics import median
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm import tqdm
from sortedcontainers import SortedList
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision.utils import save_image
import timm
from Datasets import Dataset
from EfficientNet_model import EfficientNet
import os
import boto3

# Initialize S3 client
s3_client = boto3.client('s3')

def download_file_from_s3(bucket_name, s3_key, local_path):
    s3_client.download_file(bucket_name, s3_key, local_path)

def upload_file_to_s3(local_path, bucket_name, s3_key):
    s3_client.upload_file(local_path, bucket_name, s3_key)

def get_latest_file_from_s3(bucket_name, prefix):
    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix)
    if 'Contents' not in response:
        raise ValueError("No files found in the bucket.")
    
    files = response['Contents']
    latest_file = max(files, key=lambda x: x['LastModified'])
    return latest_file['Key']

def generate_predictions(images, model, device):
    images = images.float().to(device)
    with torch.no_grad():
        outputs = model(images).clone().detach().cpu()
    return outputs

def append_predictions_to_csv(predictions, filepaths, local_output_path):
    # Convert predictions to DataFrame
    predictions_df = pd.DataFrame(predictions.numpy(), columns=['Prediction'])
    predictions_df['Filepath'] = filepaths
    if os.path.exists(local_output_path):
        existing_df = pd.read_csv(local_output_path)
        combined_df = pd.concat([existing_df, predictions_df], ignore_index=True)
    else:
        combined_df = predictions_df
    combined_df.to_csv(local_output_path, index=False)

if __name__ == '__main__':
    # Argparsing
    parser = argparse.ArgumentParser(description='Train a model.')
    parser.add_argument('--input_img_size', type=int, default=800,  help='The input image size.')
    parser.add_argument('--batch_size',     type=int, default=1,    help='The batch size of input images.')
    parser.add_argument('--model_path',     type=str, default=None, help='The path to a saved model to use.')
    parser.add_argument('--image_path',     type=str, default=None, help='The path to an image or folder of images')
    parser.add_argument('--bucket_name',    type=str, default=None, help='The S3 bucket name.')
    parser.add_argument('--input_s3_prefix', type=str, default=None, help='The S3 prefix for the input images.')
    parser.add_argument('--output_s3_key',  type=str, default=None, help='The S3 key for the output CSV.')
    args = parser.parse_args()

    input_img_size = args.input_img_size
    batch_size = args.batch_size
    model_path = args.model_path
    image_path = args.image_path
    bucket_name = args.bucket_name
    input_s3_prefix = args.input_s3_prefix
    output_s3_key = args.output_s3_key

    torch.backends.cudnn.benchmark = True
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model = EfficientNet(device, model_path, param_freeze_ratio=0.66)

    # Get the latest file from S3
    latest_file_key = get_latest_file_from_s3(bucket_name, input_s3_prefix)
    local_image_path = '/tmp/input_image.jpg'
    download_file_from_s3(bucket_name, latest_file_key, local_image_path)

    # Use the downloaded image
    image_files = [local_image_path]

    dataset_params = {
        'model_config': timm.data.resolve_model_data_config(model),
        'input_img_size': input_img_size,
        # 'ROI': ((1046,569),(1920,1440))  # RayRobertsLake
        'ROI': ((1067,417),(2393,1743))  # Pinewood
    }
    inference_dataset = Dataset(image_files, **dataset_params, scaler=model.scaler, training=False)
    dataloader = DataLoader(inference_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)

    model.eval()
    predictions = []
    filepaths = []
    for i, (images, _) in tqdm(enumerate(dataloader), total=len(dataloader)):
        outputs = generate_predictions(images, model, device)
        outputs = inference_dataset.reverse_scale(outputs)

        # Store predictions and filepaths
        predictions.extend(outputs)
        filepaths.extend(inference_dataset.image_paths[i*batch_size:(i+1)*batch_size])

    # Local path to save the predictions CSV
    local_output_path = '/tmp/inference_predictions.csv'

    # Append predictions to CSV
    append_predictions_to_csv(predictions, filepaths, local_output_path)

    # Upload the predictions CSV back to S3
    upload_file_to_s3(local_output_path, bucket_name, output_s3_key)
